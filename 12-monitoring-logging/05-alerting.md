# å‘Šè­¦ä¸å¥åº·æ£€æŸ¥

å‘Šè­¦å’Œå¥åº·æ£€æŸ¥æ˜¯ä¿éšœç³»ç»Ÿç¨³å®šæ€§çš„æœ€åä¸€é“é˜²çº¿ã€‚æœ¬æ–‡æ·±å…¥è®²è§£å‘Šè­¦ç­–ç•¥å’Œå¥åº·æ£€æŸ¥çš„è®¾è®¡ã€‚

## ç›®å½•
- [å‘Šè­¦ç­–ç•¥](#å‘Šè­¦ç­–ç•¥)
- [å‘Šè­¦æ¸ é“](#å‘Šè­¦æ¸ é“)
- [å¥åº·æ£€æŸ¥](#å¥åº·æ£€æŸ¥)
- [å‘Šè­¦è§„åˆ™](#å‘Šè­¦è§„åˆ™)
- [On-Call æœºåˆ¶](#on-call-æœºåˆ¶)
- [å‘Šè­¦é™å™ª](#å‘Šè­¦é™å™ª)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [é¢è¯•é¢˜](#å¸¸è§é¢è¯•é¢˜)

---

## å‘Šè­¦ç­–ç•¥

### å‘Šè­¦çº§åˆ«

```typescript
enum AlertSeverity {
  CRITICAL = 'critical',  // ä¸¥é‡ï¼šç«‹å³å¤„ç†ï¼Œå½±å“æœåŠ¡
  ERROR = 'error',        // é”™è¯¯ï¼šå°½å¿«å¤„ç†ï¼Œéƒ¨åˆ†åŠŸèƒ½å—å½±å“
  WARNING = 'warning',    // è­¦å‘Šï¼šéœ€è¦å…³æ³¨ï¼Œå¯èƒ½å½±å“æœªæ¥
  INFO = 'info'          // ä¿¡æ¯ï¼šä»…é€šçŸ¥
}

interface Alert {
  severity: AlertSeverity;
  title: string;
  message: string;
  tags: Record<string, string>;
  timestamp: Date;
  source: string;
}
```

### å‘Šè­¦æ¡ä»¶

```typescript
// é˜ˆå€¼å‘Šè­¦
interface ThresholdAlert {
  metric: string;
  operator: '>' | '<' | '>=' | '<=' | '==' | '!=';
  threshold: number;
  duration: number; // æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
}

// å˜åŒ–ç‡å‘Šè­¦
interface RateAlert {
  metric: string;
  change: number; // å˜åŒ–ç‡ï¼ˆ%ï¼‰
  period: number; // è§‚å¯Ÿå‘¨æœŸï¼ˆç§’ï¼‰
}

// å¼‚å¸¸æ£€æµ‹å‘Šè­¦
interface AnomalyAlert {
  metric: string;
  algorithm: 'zscore' | 'iqr' | 'prophet';
  sensitivity: number; // æ•æ„Ÿåº¦
}
```

---

## å‘Šè­¦æ¸ é“

### Slack

```typescript
import fetch from 'node-fetch';

interface SlackMessage {
  text: string;
  attachments?: any[];
  blocks?: any[];
}

class SlackNotifier {
  constructor(private webhookUrl: string) {}

  async send(alert: Alert): Promise<void> {
    const color = this.getColor(alert.severity);
    
    const message: SlackMessage = {
      text: `ğŸš¨ ${alert.title}`,
      attachments: [{
        color,
        title: alert.title,
        text: alert.message,
        fields: Object.entries(alert.tags).map(([key, value]) => ({
          title: key,
          value,
          short: true
        })),
        footer: alert.source,
        ts: Math.floor(alert.timestamp.getTime() / 1000)
      }]
    };

    await fetch(this.webhookUrl, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(message)
    });
  }

  private getColor(severity: AlertSeverity): string {
    switch (severity) {
      case AlertSeverity.CRITICAL:
        return 'danger';
      case AlertSeverity.ERROR:
        return 'warning';
      case AlertSeverity.WARNING:
        return '#ffcc00';
      default:
        return 'good';
    }
  }
}

// ä½¿ç”¨
const slack = new SlackNotifier(process.env.SLACK_WEBHOOK_URL!);

await slack.send({
  severity: AlertSeverity.CRITICAL,
  title: 'High Error Rate',
  message: 'Error rate exceeded 5% in the last 5 minutes',
  tags: {
    environment: 'production',
    service: 'api',
    error_rate: '7.2%'
  },
  timestamp: new Date(),
  source: 'Prometheus'
});
```

### Email

```typescript
import nodemailer from 'nodemailer';

class EmailNotifier {
  private transporter: nodemailer.Transporter;

  constructor() {
    this.transporter = nodemailer.createTransporter({
      host: process.env.SMTP_HOST,
      port: parseInt(process.env.SMTP_PORT || '587'),
      secure: false,
      auth: {
        user: process.env.SMTP_USER,
        pass: process.env.SMTP_PASS
      }
    });
  }

  async send(alert: Alert, recipients: string[]): Promise<void> {
    const html = this.renderHtml(alert);

    await this.transporter.sendMail({
      from: process.env.ALERT_FROM_EMAIL,
      to: recipients.join(','),
      subject: `[${alert.severity.toUpperCase()}] ${alert.title}`,
      html
    });
  }

  private renderHtml(alert: Alert): string {
    const color = alert.severity === AlertSeverity.CRITICAL ? '#ff0000' : '#ff9900';

    return `
      <div style="font-family: Arial, sans-serif;">
        <div style="background-color: ${color}; color: white; padding: 20px;">
          <h2>${alert.title}</h2>
        </div>
        <div style="padding: 20px;">
          <p>${alert.message}</p>
          <table>
            ${Object.entries(alert.tags).map(([key, value]) => `
              <tr>
                <td style="font-weight: bold; padding: 5px;">${key}:</td>
                <td style="padding: 5px;">${value}</td>
              </tr>
            `).join('')}
          </table>
          <p style="color: #666; margin-top: 20px;">
            ${alert.source} | ${alert.timestamp.toISOString()}
          </p>
        </div>
      </div>
    `;
  }
}
```

### SMSï¼ˆTwilioï¼‰

```typescript
import twilio from 'twilio';

class SMSNotifier {
  private client: twilio.Twilio;

  constructor() {
    this.client = twilio(
      process.env.TWILIO_ACCOUNT_SID,
      process.env.TWILIO_AUTH_TOKEN
    );
  }

  async send(alert: Alert, phoneNumbers: string[]): Promise<void> {
    const message = `[${alert.severity.toUpperCase()}] ${alert.title}\n${alert.message}`;

    for (const phone of phoneNumbers) {
      await this.client.messages.create({
        body: message,
        from: process.env.TWILIO_PHONE_NUMBER,
        to: phone
      });
    }
  }
}
```

### PagerDuty

```typescript
class PagerDutyNotifier {
  constructor(private routingKey: string) {}

  async send(alert: Alert): Promise<void> {
    await fetch('https://events.pagerduty.com/v2/enqueue', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        routing_key: this.routingKey,
        event_action: 'trigger',
        dedup_key: this.generateDedupKey(alert),
        payload: {
          summary: alert.title,
          severity: this.mapSeverity(alert.severity),
          source: alert.source,
          timestamp: alert.timestamp.toISOString(),
          custom_details: {
            message: alert.message,
            tags: alert.tags
          }
        }
      })
    });
  }

  private generateDedupKey(alert: Alert): string {
    // ç”¨äºå»é‡ç›¸åŒçš„å‘Šè­¦
    return `${alert.source}-${alert.title}-${JSON.stringify(alert.tags)}`;
  }

  private mapSeverity(severity: AlertSeverity): string {
    const mapping = {
      [AlertSeverity.CRITICAL]: 'critical',
      [AlertSeverity.ERROR]: 'error',
      [AlertSeverity.WARNING]: 'warning',
      [AlertSeverity.INFO]: 'info'
    };
    return mapping[severity];
  }

  async resolve(dedupKey: string): Promise<void> {
    await fetch('https://events.pagerduty.com/v2/enqueue', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        routing_key: this.routingKey,
        event_action: 'resolve',
        dedup_key: dedupKey
      })
    });
  }
}
```

### å¤šæ¸ é“å‘Šè­¦

```typescript
class AlertManager {
  private notifiers: Map<string, Notifier> = new Map();

  registerNotifier(name: string, notifier: Notifier): void {
    this.notifiers.set(name, notifier);
  }

  async sendAlert(alert: Alert, channels: string[]): Promise<void> {
    const promises = channels.map(async (channel) => {
      const notifier = this.notifiers.get(channel);
      if (!notifier) {
        console.warn(`Notifier ${channel} not found`);
        return;
      }

      try {
        await notifier.send(alert);
      } catch (error) {
        console.error(`Failed to send alert via ${channel}:`, error);
      }
    });

    await Promise.allSettled(promises);
  }

  // æ ¹æ®ä¸¥é‡ç¨‹åº¦é€‰æ‹©æ¸ é“
  async sendAlertBySeverity(alert: Alert): Promise<void> {
    let channels: string[];

    switch (alert.severity) {
      case AlertSeverity.CRITICAL:
        channels = ['pagerduty', 'slack', 'sms'];
        break;
      case AlertSeverity.ERROR:
        channels = ['slack', 'email'];
        break;
      case AlertSeverity.WARNING:
        channels = ['slack'];
        break;
      default:
        channels = [];
    }

    await this.sendAlert(alert, channels);
  }
}

// ä½¿ç”¨
const alertManager = new AlertManager();

alertManager.registerNotifier('slack', new SlackNotifier(process.env.SLACK_WEBHOOK_URL!));
alertManager.registerNotifier('email', new EmailNotifier());
alertManager.registerNotifier('sms', new SMSNotifier());
alertManager.registerNotifier('pagerduty', new PagerDutyNotifier(process.env.PAGERDUTY_ROUTING_KEY!));

// å‘é€å‘Šè­¦
await alertManager.sendAlertBySeverity({
  severity: AlertSeverity.CRITICAL,
  title: 'Service Down',
  message: 'API service is not responding',
  tags: {
    environment: 'production',
    service: 'api'
  },
  timestamp: new Date(),
  source: 'Kubernetes'
});
```

---

## å¥åº·æ£€æŸ¥

### åŸºç¡€å¥åº·æ£€æŸ¥

```typescript
import express from 'express';

const app = express();

// Liveness Probeï¼ˆå­˜æ´»æ¢é’ˆï¼‰
app.get('/health/live', (req, res) => {
  // ç®€å•è¿”å› 200ï¼Œè¡¨ç¤ºè¿›ç¨‹è¿˜æ´»ç€
  res.status(200).json({ status: 'ok' });
});

// Readiness Probeï¼ˆå°±ç»ªæ¢é’ˆï¼‰
app.get('/health/ready', async (req, res) => {
  try {
    // æ£€æŸ¥ä¾èµ–æœåŠ¡
    await Promise.all([
      checkDatabase(),
      checkRedis(),
      checkExternalAPI()
    ]);

    res.status(200).json({
      status: 'ready',
      timestamp: new Date().toISOString()
    });
  } catch (error) {
    res.status(503).json({
      status: 'not_ready',
      error: error.message,
      timestamp: new Date().toISOString()
    });
  }
});

async function checkDatabase(): Promise<void> {
  await prisma.$queryRaw`SELECT 1`;
}

async function checkRedis(): Promise<void> {
  await redis.ping();
}

async function checkExternalAPI(): Promise<void> {
  const response = await fetch('https://api.example.com/health', {
    timeout: 3000
  });
  
  if (!response.ok) {
    throw new Error('External API is down');
  }
}
```

### è¯¦ç»†å¥åº·æ£€æŸ¥

```typescript
interface HealthCheckResult {
  status: 'healthy' | 'degraded' | 'unhealthy';
  timestamp: string;
  version: string;
  uptime: number;
  checks: Record<string, CheckResult>;
}

interface CheckResult {
  status: 'pass' | 'fail' | 'warn';
  message?: string;
  duration?: number;
  metadata?: Record<string, any>;
}

class HealthChecker {
  private checks: Map<string, () => Promise<CheckResult>> = new Map();

  register(name: string, check: () => Promise<CheckResult>): void {
    this.checks.set(name, check);
  }

  async check(): Promise<HealthCheckResult> {
    const results: Record<string, CheckResult> = {};
    const promises: Promise<void>[] = [];

    for (const [name, check] of this.checks.entries()) {
      promises.push(
        (async () => {
          const start = Date.now();
          try {
            results[name] = await Promise.race([
              check(),
              this.timeout(5000) // 5 ç§’è¶…æ—¶
            ]);
            results[name].duration = Date.now() - start;
          } catch (error) {
            results[name] = {
              status: 'fail',
              message: error.message,
              duration: Date.now() - start
            };
          }
        })()
      );
    }

    await Promise.all(promises);

    const status = this.determineOverallStatus(results);

    return {
      status,
      timestamp: new Date().toISOString(),
      version: process.env.APP_VERSION || 'unknown',
      uptime: process.uptime(),
      checks: results
    };
  }

  private determineOverallStatus(
    checks: Record<string, CheckResult>
  ): 'healthy' | 'degraded' | 'unhealthy' {
    const statuses = Object.values(checks).map(c => c.status);

    if (statuses.every(s => s === 'pass')) {
      return 'healthy';
    }

    if (statuses.some(s => s === 'fail')) {
      return 'unhealthy';
    }

    return 'degraded';
  }

  private async timeout(ms: number): Promise<CheckResult> {
    await new Promise((resolve) => setTimeout(resolve, ms));
    return {
      status: 'fail',
      message: 'Health check timeout'
    };
  }
}

// ä½¿ç”¨
const healthChecker = new HealthChecker();

// æ•°æ®åº“æ£€æŸ¥
healthChecker.register('database', async () => {
  try {
    const start = Date.now();
    await prisma.$queryRaw`SELECT 1`;
    const duration = Date.now() - start;

    return {
      status: duration < 100 ? 'pass' : 'warn',
      message: duration < 100 ? 'Database is healthy' : 'Database is slow',
      metadata: { latency_ms: duration }
    };
  } catch (error) {
    return {
      status: 'fail',
      message: error.message
    };
  }
});

// Redis æ£€æŸ¥
healthChecker.register('redis', async () => {
  try {
    await redis.ping();
    return {
      status: 'pass',
      message: 'Redis is healthy'
    };
  } catch (error) {
    return {
      status: 'fail',
      message: error.message
    };
  }
});

// ç£ç›˜ç©ºé—´æ£€æŸ¥
healthChecker.register('disk', async () => {
  const diskUsage = await checkDiskUsage();
  
  if (diskUsage < 80) {
    return {
      status: 'pass',
      message: 'Disk space is sufficient',
      metadata: { usage_percent: diskUsage }
    };
  } else if (diskUsage < 90) {
    return {
      status: 'warn',
      message: 'Disk space is running low',
      metadata: { usage_percent: diskUsage }
    };
  } else {
    return {
      status: 'fail',
      message: 'Disk space is critically low',
      metadata: { usage_percent: diskUsage }
    };
  }
});

// å†…å­˜æ£€æŸ¥
healthChecker.register('memory', async () => {
  const used = process.memoryUsage();
  const heapUsedPercent = (used.heapUsed / used.heapTotal) * 100;

  if (heapUsedPercent < 80) {
    return {
      status: 'pass',
      message: 'Memory usage is normal',
      metadata: {
        heap_used_mb: Math.round(used.heapUsed / 1024 / 1024),
        heap_total_mb: Math.round(used.heapTotal / 1024 / 1024),
        usage_percent: Math.round(heapUsedPercent)
      }
    };
  } else {
    return {
      status: 'warn',
      message: 'Memory usage is high',
      metadata: {
        heap_used_mb: Math.round(used.heapUsed / 1024 / 1024),
        heap_total_mb: Math.round(used.heapTotal / 1024 / 1024),
        usage_percent: Math.round(heapUsedPercent)
      }
    };
  }
});

// å¥åº·æ£€æŸ¥ç«¯ç‚¹
app.get('/health', async (req, res) => {
  const result = await healthChecker.check();
  
  const statusCode = result.status === 'healthy' ? 200 :
                     result.status === 'degraded' ? 200 : 503;
  
  res.status(statusCode).json(result);
});
```

### Kubernetes å¥åº·æ£€æŸ¥

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 3
  template:
    spec:
      containers:
        - name: app
          image: my-app:1.0.0
          ports:
            - containerPort: 3000
          
          # å­˜æ´»æ¢é’ˆ
          livenessProbe:
            httpGet:
              path: /health/live
              port: 3000
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          
          # å°±ç»ªæ¢é’ˆ
          readinessProbe:
            httpGet:
              path: /health/ready
              port: 3000
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3
          
          # å¯åŠ¨æ¢é’ˆ
          startupProbe:
            httpGet:
              path: /health/live
              port: 3000
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 3
            failureThreshold: 30
```

---

## å‘Šè­¦è§„åˆ™

### Prometheus å‘Šè­¦

```yaml
# alerts.yml
groups:
  - name: api_alerts
    interval: 30s
    rules:
      # æœåŠ¡ä¸å¯ç”¨
      - alert: ServiceDown
        expr: up{job="api"} == 0
        for: 1m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "Service {{ $labels.instance }} is down"
          description: "{{ $labels.instance }} has been down for more than 1 minute"
          runbook_url: "https://wiki.example.com/runbooks/service-down"

      # é«˜é”™è¯¯ç‡
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status_code=~"5.."}[5m]))
            / 
            sum(rate(http_requests_total[5m]))
          ) > 0.05
        for: 5m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"
          dashboard_url: "https://grafana.example.com/d/errors"

      # é«˜å»¶è¿Ÿ
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (le, route)
          ) > 1
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "High latency on {{ $labels.route }}"
          description: "P95 latency is {{ $value }}s (threshold: 1s)"

      # CPU ä½¿ç”¨ç‡é«˜
      - alert: HighCPU
        expr: |
          100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 10m
        labels:
          severity: warning
          team: infra
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value | humanize }}% (threshold: 80%)"

      # å†…å­˜ä½¿ç”¨ç‡é«˜
      - alert: HighMemory
        expr: |
          (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
        for: 5m
        labels:
          severity: critical
          team: infra
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value | humanize }}% (threshold: 90%)"

      # ç£ç›˜ç©ºé—´ä¸è¶³
      - alert: LowDiskSpace
        expr: |
          (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lxcfs"} / node_filesystem_size_bytes) * 100 < 10
        for: 5m
        labels:
          severity: warning
          team: infra
        annotations:
          summary: "Low disk space on {{ $labels.instance }}"
          description: "Only {{ $value | humanize }}% disk space left (threshold: 10%)"

      # æ•°æ®åº“è¿æ¥æ± è€—å°½
      - alert: DatabasePoolExhausted
        expr: db_pool_idle_connections == 0
        for: 2m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "Database connection pool exhausted"
          description: "No idle database connections available"

      # é˜Ÿåˆ—ç§¯å‹
      - alert: QueueBacklog
        expr: queue_size > 1000
        for: 10m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "Queue backlog detected"
          description: "Queue {{ $labels.queue_name }} has {{ $value }} pending items"
```

### Alertmanager é…ç½®

```yaml
# alertmanager.yml
global:
  resolve_timeout: 5m
  slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'

# è·¯ç”±è§„åˆ™
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'
  
  routes:
    # ä¸¥é‡å‘Šè­¦ -> PagerDuty + Slack
    - match:
        severity: critical
      receiver: 'pagerduty-critical'
      continue: true
    
    - match:
        severity: critical
      receiver: 'slack-critical'
    
    # è­¦å‘Š -> Slack
    - match:
        severity: warning
      receiver: 'slack-warnings'
    
    # åŸºç¡€è®¾æ–½å›¢é˜Ÿ
    - match:
        team: infra
      receiver: 'slack-infra'

# æ¥æ”¶å™¨
receivers:
  - name: 'default'
    slack_configs:
      - channel: '#alerts'
        title: '{{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

  - name: 'pagerduty-critical'
    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        description: '{{ .GroupLabels.alertname }}'
        details:
          firing: '{{ .Alerts.Firing | len }}'
          resolved: '{{ .Alerts.Resolved | len }}'

  - name: 'slack-critical'
    slack_configs:
      - channel: '#critical-alerts'
        color: 'danger'
        title: 'ğŸš¨ {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

  - name: 'slack-warnings'
    slack_configs:
      - channel: '#warnings'
        color: 'warning'
        title: 'âš ï¸ {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

  - name: 'slack-infra'
    slack_configs:
      - channel: '#infra-alerts'
        title: '{{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

# æŠ‘åˆ¶è§„åˆ™
inhibit_rules:
  # å¦‚æœæœåŠ¡å®•æœºï¼ŒæŠ‘åˆ¶å…¶ä»–å‘Šè­¦
  - source_match:
      alertname: 'ServiceDown'
    target_match_re:
      alertname: 'High.*'
    equal: ['instance']
```

---

## On-Call æœºåˆ¶

### è½®å€¼è¡¨

```typescript
interface OnCallSchedule {
  id: string;
  team: string;
  rotations: Rotation[];
}

interface Rotation {
  id: string;
  start: Date;
  end: Date;
  primary: User;
  secondary?: User;
}

interface User {
  id: string;
  name: string;
  email: string;
  phone: string;
  timezone: string;
}

class OnCallManager {
  async getCurrentOnCall(team: string): Promise<Rotation | null> {
    const schedule = await this.getSchedule(team);
    const now = new Date();

    return schedule.rotations.find(
      rotation => now >= rotation.start && now <= rotation.end
    ) || null;
  }

  async escalate(alert: Alert, team: string): Promise<void> {
    const rotation = await this.getCurrentOnCall(team);

    if (!rotation) {
      throw new Error('No on-call person found');
    }

    // 1. é€šçŸ¥ä¸»è¦å€¼ç­äººå‘˜
    await this.notifyUser(rotation.primary, alert, 'primary');

    // 2. ç­‰å¾… 5 åˆ†é’Ÿ
    await this.sleep(5 * 60 * 1000);

    // 3. å¦‚æœæœªç¡®è®¤ï¼Œå‡çº§åˆ°å¤‡ç”¨äººå‘˜
    if (!await this.isAcknowledged(alert)) {
      if (rotation.secondary) {
        await this.notifyUser(rotation.secondary, alert, 'escalated');
      } else {
        // å‡çº§åˆ°å›¢é˜Ÿè´Ÿè´£äºº
        await this.notifyTeamLead(team, alert);
      }
    }
  }

  private async notifyUser(user: User, alert: Alert, level: string): Promise<void> {
    // å‘é€å¤šæ¸ é“é€šçŸ¥
    await Promise.all([
      this.sendSMS(user.phone, alert),
      this.sendEmail(user.email, alert),
      this.callPhone(user.phone, alert) // ä¸¥é‡å‘Šè­¦æ—¶æ‰“ç”µè¯
    ]);
  }
}
```

---

## å‘Šè­¦é™å™ª

### å‘Šè­¦èšåˆ

```typescript
class AlertAggregator {
  private buffer: Map<string, Alert[]> = new Map();
  private flushInterval = 60000; // 1 åˆ†é’Ÿ

  constructor() {
    setInterval(() => this.flush(), this.flushInterval);
  }

  add(alert: Alert): void {
    const key = this.getGroupKey(alert);
    
    if (!this.buffer.has(key)) {
      this.buffer.set(key, []);
    }
    
    this.buffer.get(key)!.push(alert);
  }

  private getGroupKey(alert: Alert): string {
    // æŒ‰æœåŠ¡å’Œå‘Šè­¦ç±»å‹åˆ†ç»„
    return `${alert.tags.service}-${alert.title}`;
  }

  private async flush(): Promise<void> {
    for (const [key, alerts] of this.buffer.entries()) {
      if (alerts.length === 0) continue;

      if (alerts.length === 1) {
        // å•ä¸ªå‘Šè­¦ï¼Œæ­£å¸¸å‘é€
        await this.sendAlert(alerts[0]);
      } else {
        // å¤šä¸ªå‘Šè­¦ï¼Œèšåˆå‘é€
        await this.sendAggregatedAlert(key, alerts);
      }
    }

    this.buffer.clear();
  }

  private async sendAggregatedAlert(key: string, alerts: Alert[]): Promise<void> {
    const aggregated: Alert = {
      severity: this.getHighestSeverity(alerts),
      title: `${alerts.length} alerts: ${key}`,
      message: this.aggregateMessages(alerts),
      tags: alerts[0].tags,
      timestamp: new Date(),
      source: alerts[0].source
    };

    await alertManager.sendAlert(aggregated, ['slack']);
  }

  private getHighestSeverity(alerts: Alert[]): AlertSeverity {
    const severityOrder = {
      [AlertSeverity.CRITICAL]: 4,
      [AlertSeverity.ERROR]: 3,
      [AlertSeverity.WARNING]: 2,
      [AlertSeverity.INFO]: 1
    };

    return alerts.reduce((highest, alert) => {
      return severityOrder[alert.severity] > severityOrder[highest]
        ? alert.severity
        : highest;
    }, AlertSeverity.INFO);
  }

  private aggregateMessages(alerts: Alert[]): string {
    const counts = new Map<string, number>();
    
    for (const alert of alerts) {
      counts.set(alert.message, (counts.get(alert.message) || 0) + 1);
    }

    return Array.from(counts.entries())
      .map(([message, count]) => `${message} (${count}x)`)
      .join('\n');
  }
}
```

### å‘Šè­¦é™é»˜

```typescript
interface SilenceRule {
  id: string;
  matchers: Record<string, string>;
  start: Date;
  end: Date;
  creator: string;
  comment: string;
}

class AlertSilencer {
  private silences: SilenceRule[] = [];

  addSilence(rule: SilenceRule): void {
    this.silences.push(rule);
  }

  removeSilence(id: string): void {
    this.silences = this.silences.filter(s => s.id !== id);
  }

  shouldSilence(alert: Alert): boolean {
    const now = new Date();

    return this.silences.some(silence => {
      // æ£€æŸ¥æ—¶é—´èŒƒå›´
      if (now < silence.start || now > silence.end) {
        return false;
      }

      // æ£€æŸ¥åŒ¹é…æ¡ä»¶
      return Object.entries(silence.matchers).every(([key, value]) => {
        return alert.tags[key] === value;
      });
    });
  }
}

// ä½¿ç”¨
const silencer = new AlertSilencer();

// ç»´æŠ¤æœŸé—´é™é»˜å‘Šè­¦
silencer.addSilence({
  id: 'maintenance-1',
  matchers: {
    service: 'api',
    environment: 'production'
  },
  start: new Date('2024-01-01T02:00:00Z'),
  end: new Date('2024-01-01T04:00:00Z'),
  creator: 'admin',
  comment: 'Database maintenance'
});

// å‘é€å‘Šè­¦å‰æ£€æŸ¥
async function sendAlert(alert: Alert): Promise<void> {
  if (silencer.shouldSilence(alert)) {
    console.log('Alert silenced:', alert.title);
    return;
  }

  await alertManager.sendAlert(alert, ['slack']);
}
```

---

## æœ€ä½³å®è·µ

### 1. å‘Šè­¦åŸåˆ™

```typescript
// âœ… å¥½çš„å‘Šè­¦
// - å¯æ“ä½œï¼šæ”¶åˆ°å‘Šè­¦åçŸ¥é“è¯¥åšä»€ä¹ˆ
// - æœ‰æ„ä¹‰ï¼šå‘Šè­¦çœŸçš„è¡¨ç¤ºæœ‰é—®é¢˜
// - åŠæ—¶ï¼šé—®é¢˜å‘ç”Ÿæ—¶ç«‹å³å‘Šè­¦
// - å‡†ç¡®ï¼šä½è¯¯æŠ¥ç‡

// âŒ ä¸å¥½çš„å‘Šè­¦
// - å¤ªå¤šï¼šå‘Šè­¦ç–²åŠ³
// - å¤ªå°‘ï¼šé—®é¢˜æœªè¢«å‘ç°
// - æ¨¡ç³Šï¼šä¸çŸ¥é“é—®é¢˜åœ¨å“ª
// - å»¶è¿Ÿï¼šé—®é¢˜å·²ç»è¿‡å»æ‰å‘Šè­¦
```

### 2. Runbook

```typescript
// æ¯ä¸ªå‘Šè­¦éƒ½åº”è¯¥æœ‰å¯¹åº”çš„ Runbook
interface Runbook {
  alert: string;
  severity: AlertSeverity;
  description: string;
  impact: string;
  diagnosis: string[];
  resolution: string[];
  escalation: string;
}

const runbooks: Map<string, Runbook> = new Map([
  ['HighErrorRate', {
    alert: 'HighErrorRate',
    severity: AlertSeverity.CRITICAL,
    description: 'API é”™è¯¯ç‡è¶…è¿‡ 5%',
    impact: 'å½±å“ç”¨æˆ·ä½“éªŒï¼Œéƒ¨åˆ†è¯·æ±‚å¤±è´¥',
    diagnosis: [
      '1. æ£€æŸ¥ Grafana Dashboard æŸ¥çœ‹é”™è¯¯åˆ†å¸ƒ',
      '2. æŸ¥çœ‹ Sentry æœ€æ–°é”™è¯¯',
      '3. æ£€æŸ¥æœ€è¿‘çš„éƒ¨ç½²',
      '4. æŸ¥çœ‹æ•°æ®åº“æ€§èƒ½'
    ],
    resolution: [
      '1. å¦‚æœæ˜¯æ–°éƒ¨ç½²å¯¼è‡´ï¼Œå›æ»šåˆ°ä¸Šä¸€ä¸ªç‰ˆæœ¬',
      '2. å¦‚æœæ˜¯æ•°æ®åº“é—®é¢˜ï¼Œæ£€æŸ¥æ…¢æŸ¥è¯¢',
      '3. å¦‚æœæ˜¯å¤–éƒ¨æœåŠ¡é—®é¢˜ï¼Œå¯ç”¨é™çº§æ–¹æ¡ˆ',
      '4. ä¿®å¤ä»£ç åé‡æ–°éƒ¨ç½²'
    ],
    escalation: 'å¦‚æœ 30 åˆ†é’Ÿå†…æ— æ³•è§£å†³ï¼Œå‡çº§åˆ° Team Lead'
  }]
]);
```

### 3. å‘Šè­¦æµ‹è¯•

```typescript
// å®šæœŸæµ‹è¯•å‘Šè­¦ç³»ç»Ÿ
async function testAlertSystem(): Promise<void> {
  const testAlert: Alert = {
    severity: AlertSeverity.WARNING,
    title: 'Alert System Test',
    message: 'This is a test alert',
    tags: {
      test: 'true',
      environment: 'test'
    },
    timestamp: new Date(),
    source: 'test'
  };

  await alertManager.sendAlert(testAlert, ['slack']);
  
  console.log('Test alert sent successfully');
}

// æ¯å‘¨æµ‹è¯•ä¸€æ¬¡
setInterval(testAlertSystem, 7 * 24 * 60 * 60 * 1000);
```

---

## å¸¸è§é¢è¯•é¢˜

### 1. å¦‚ä½•è®¾è®¡å‘Šè­¦ç³»ç»Ÿï¼Ÿ

**è¦ç‚¹**ï¼š

1. **å‘Šè­¦çº§åˆ«**ï¼šCriticalã€Errorã€Warningã€Info
2. **å‘Šè­¦æ¸ é“**ï¼šSlackã€Emailã€SMSã€PagerDuty
3. **å‘Šè­¦è·¯ç”±**ï¼šæ ¹æ®çº§åˆ«å’Œå›¢é˜Ÿè·¯ç”±
4. **å‘Šè­¦èšåˆ**ï¼šé¿å…å‘Šè­¦é£æš´
5. **å‘Šè­¦é™é»˜**ï¼šç»´æŠ¤æœŸé—´é™é»˜
6. **å‘Šè­¦å‡çº§**ï¼šæœªç¡®è®¤æ—¶å‡çº§
7. **Runbook**ï¼šæ¯ä¸ªå‘Šè­¦éƒ½æœ‰å¤„ç†æ–‡æ¡£

### 2. å¦‚ä½•é¿å…å‘Šè­¦ç–²åŠ³ï¼Ÿ

**æ–¹æ³•**ï¼š

1. **å‡å°‘å™ªéŸ³**ï¼š
   - æé«˜å‘Šè­¦é˜ˆå€¼
   - å¢åŠ æŒç»­æ—¶é—´
   - è¿‡æ»¤å·²çŸ¥é—®é¢˜

2. **å‘Šè­¦èšåˆ**ï¼š
   - ç›¸åŒç±»å‹å‘Šè­¦åˆå¹¶
   - æ‰¹é‡é€šçŸ¥

3. **å‘Šè­¦ä¼˜å…ˆçº§**ï¼š
   - åªæœ‰é‡è¦å‘Šè­¦æ‰é€šçŸ¥
   - ä¸é‡è¦çš„è®°å½•æ—¥å¿—

4. **è‡ªåŠ¨åŒ–**ï¼š
   - è‡ªåŠ¨ä¿®å¤å¸¸è§é—®é¢˜
   - è‡ªåŠ¨é™é»˜é‡å¤å‘Šè­¦

5. **å®šæœŸå®¡æŸ¥**ï¼š
   - æ£€æŸ¥å‘Šè­¦æœ‰æ•ˆæ€§
   - åˆ é™¤æ— ç”¨å‘Šè­¦

### 3. Liveness vs Readinessï¼Ÿ

| æ¢é’ˆ | ä½œç”¨ | å¤±è´¥åæœ |
|------|------|---------|
| **Liveness** | æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å­˜æ´» | é‡å¯å®¹å™¨ |
| **Readiness** | æ£€æŸ¥æ˜¯å¦å¯ä»¥æ¥æ”¶æµé‡ | ä»è´Ÿè½½å‡è¡¡ç§»é™¤ |
| **Startup** | æ£€æŸ¥æ˜¯å¦å¯åŠ¨å®Œæˆ | ç­‰å¾…æˆ–é‡å¯ |

**ç¤ºä¾‹**ï¼š
- Livenessï¼šæ£€æŸ¥è¿›ç¨‹æ˜¯å¦æ­»é”
- Readinessï¼šæ£€æŸ¥æ•°æ®åº“è¿æ¥æ˜¯å¦æ­£å¸¸
- Startupï¼šæ£€æŸ¥åˆå§‹åŒ–æ˜¯å¦å®Œæˆ

### 4. å¦‚ä½•å¤„ç†å‘Šè­¦é£æš´ï¼Ÿ

**æ­¥éª¤**ï¼š

1. **è¯†åˆ«æ ¹å› **ï¼š
   - æŸ¥çœ‹å‘Šè­¦æ—¶é—´çº¿
   - è¯†åˆ«æœ€æ—©çš„å‘Šè­¦

2. **ä¸´æ—¶é™é»˜**ï¼š
   - é™é»˜è¡ç”Ÿå‘Šè­¦
   - ä¿ç•™æ ¹å› å‘Šè­¦

3. **å¿«é€Ÿä¿®å¤**ï¼š
   - ä¼˜å…ˆä¿®å¤æ ¹å› 
   - æˆ–å¯ç”¨é™çº§æ–¹æ¡ˆ

4. **äº‹ååˆ†æ**ï¼š
   - ä¸ºä»€ä¹ˆä¼šäº§ç”Ÿå‘Šè­¦é£æš´
   - å¦‚ä½•æ”¹è¿›å‘Šè­¦è§„åˆ™

### 5. On-Call æœ€ä½³å®è·µï¼Ÿ

**å®è·µ**ï¼š

1. **æ˜ç¡®è½®å€¼**ï¼š
   - æ¸…æ™°çš„è½®å€¼è¡¨
   - è‡ªåŠ¨åŒ–è½®å€¼æé†’

2. **åˆç†è´Ÿè½½**ï¼š
   - ä¸è¦å¤ªé¢‘ç¹
   - åˆç†çš„ä¼‘æ¯æ—¶é—´

3. **å·¥å…·æ”¯æŒ**ï¼š
   - PagerDuty ç­‰ On-Call å·¥å…·
   - ç§»åŠ¨åº”ç”¨

4. **Runbook**ï¼š
   - æ¯ä¸ªå‘Šè­¦éƒ½æœ‰å¤„ç†æ–‡æ¡£
   - å®šæœŸæ›´æ–°

5. **äº‹åæ€»ç»“**ï¼š
   - è®°å½•å¤„ç†è¿‡ç¨‹
   - æ”¹è¿›å‘Šè­¦å’Œæµç¨‹

---

## æ€»ç»“

### å‘Šè­¦ç³»ç»Ÿæ¶æ„

```
å‘Šè­¦æºï¼ˆPrometheusã€Sentryã€è‡ªå®šä¹‰ï¼‰
    â†“
å‘Šè­¦èšåˆä¸å»é‡
    â†“
å‘Šè­¦è·¯ç”±ï¼ˆæŒ‰çº§åˆ«ã€å›¢é˜Ÿï¼‰
    â†“
å‘Šè­¦æ¸ é“ï¼ˆSlackã€Emailã€SMSã€PagerDutyï¼‰
    â†“
On-Call å‡çº§
```

### å®è·µæ£€æŸ¥æ¸…å•

- [ ] è®¾ç½®å¥åº·æ£€æŸ¥ç«¯ç‚¹
- [ ] é…ç½®å‘Šè­¦è§„åˆ™
- [ ] é›†æˆå‘Šè­¦æ¸ é“
- [ ] ç¼–å†™ Runbook
- [ ] è®¾ç½® On-Call è½®å€¼
- [ ] å®æ–½å‘Šè­¦èšåˆ
- [ ] é…ç½®å‘Šè­¦é™é»˜
- [ ] å®šæœŸæµ‹è¯•å‘Šè­¦ç³»ç»Ÿ
- [ ] å®¡æŸ¥å‘Šè­¦æœ‰æ•ˆæ€§
- [ ] ä¼˜åŒ–å‘Šè­¦é˜ˆå€¼

---

**ä¸Šä¸€ç¯‡**ï¼š[é”™è¯¯è¿½è¸ª](./04-error-tracking.md)  
**è¿”å›ç›®å½•**ï¼š[ç›‘æ§ä¸æ—¥å¿—](./README.md)

